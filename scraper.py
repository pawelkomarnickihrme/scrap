#!/usr/bin/env python3
"""
Program do scrapowania danych o perfumach ze strony.
UÅ¼ywa Crawl4AI i BeautifulSoup do pobrania i parsowania danych.
"""

import json
import re
import sys
import random
import asyncio
from typing import Dict, List, Optional, Any
from urllib.parse import urljoin

from bs4 import BeautifulSoup, Tag, NavigableString
from crawl4ai import AsyncWebCrawler
from vpn_manager import VPNManager


# Lista User-Agent do rotacji (rÃ³Å¼ne przeglÄ…darki i systemy)
USER_AGENTS = [
    # Chrome na Windows
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
    # Chrome na macOS
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
    # Firefox na Windows
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:120.0) Gecko/20100101 Firefox/120.0",
    # Firefox na macOS
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:121.0) Gecko/20100101 Firefox/121.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:120.0) Gecko/20100101 Firefox/120.0",
    # Safari na macOS
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15",
    # Edge na Windows
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36 Edg/119.0.0.0",
]


def get_random_headers() -> Dict[str, str]:
    """Generuje losowe nagÅ‚Ã³wki HTTP z rotacjÄ… User-Agent."""
    user_agent = random.choice(USER_AGENTS)
    
    # RÃ³Å¼ne Accept-Language w zaleÅ¼noÅ›ci od User-Agent
    if "Firefox" in user_agent:
        accept_language = random.choice([
            "pl-PL,pl;q=0.9,en-US;q=0.8,en;q=0.7",
            "en-US,en;q=0.9",
            "pl-PL,pl;q=0.9",
        ])
    else:
        accept_language = random.choice([
            "pl-PL,pl;q=0.9,en-US;q=0.8,en;q=0.7",
            "en-US,en;q=0.9,pl;q=0.8",
            "pl-PL,pl;q=0.9",
        ])
    
    headers = {
        "User-Agent": user_agent,
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
        "Accept-Language": accept_language,
        "Accept-Encoding": "gzip, deflate, br",
        "DNT": "1",
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1",
        "Sec-Fetch-Dest": "document",
        "Sec-Fetch-Mode": "navigate",
        "Sec-Fetch-Site": random.choice(["none", "same-origin"]),
        "Sec-Fetch-User": "?1",
        "Cache-Control": random.choice(["max-age=0", "no-cache", "no-store"]),
        "Referer": random.choice([
            "https://www.google.com/",
            "https://www.google.pl/",
            "https://www.fragrantica.com/",
            "",
        ]),
    }
    
    return headers


def clean_text(text: str) -> str:
    """Usuwa biaÅ‚e znaki i normalizuje tekst."""
    if not text:
        return ""
    return " ".join(text.split())


def is_404_error_page(html: str, status_code: int = None) -> bool:
    """Sprawdza czy strona jest stronÄ… bÅ‚Ä™du 404.

    Args:
        html: ZawartoÅ›Ä‡ HTML strony
        status_code: Kod statusu HTTP (jeÅ›li dostÄ™pny)

    Returns:
        True jeÅ›li strona jest bÅ‚Ä™dem 404, False w przeciwnym razie
    """
    # JeÅ›li mamy kod statusu 404, to na pewno bÅ‚Ä…d
    if status_code == 404:
        return True

    # JeÅ›li mamy inny kod statusu bÅ‚Ä™du (4xx, 5xx), prawdopodobnie bÅ‚Ä…d
    if status_code and status_code >= 400:
        return True

    # JeÅ›li nie mamy kodu statusu lub jest 200, sprawdzamy zawartoÅ›Ä‡ HTML
    if not html:
        return False

    soup = BeautifulSoup(html, "html.parser")
    html_lower = html.lower()

    # SprawdÅº tytuÅ‚ strony
    title = soup.find("title")
    if title:
        title_text = clean_text(title.get_text()).lower()
        if "404" in title_text or "not found" in title_text or "page not found" in title_text:
            return True

    # SprawdÅº czy strona zawiera charakterystyczne elementy bÅ‚Ä™dÃ³w 404
    # Szukaj specyficznych wzorcÃ³w bÅ‚Ä™dÃ³w 404, nie tylko sÅ‚Ã³w
    error_indicators = [
        "404 error",
        "page not found",
        "the page you are looking for",
        "this page doesn't exist",
        "error 404",
        "http 404",
        "404 - not found"
    ]

    for indicator in error_indicators:
        if indicator in html_lower:
            return True

    # SprawdÅº czy strona jest bardzo krÃ³tka (typowe dla stron bÅ‚Ä™dÃ³w)
    # ale zawiera sÅ‚owa kluczowe bÅ‚Ä™dÃ³w w widocznej treÅ›ci (nie w JavaScript)
    if len(html) < 2000 and ("error" in html_lower or "not found" in html_lower):
        # Dodatkowe sprawdzenie - czy to nie jest normalna strona zawierajÄ…ca te sÅ‚owa
        # w JavaScript lub innych niewidocznych elementach
        body = soup.find("body")
        if body:
            # UsuÅ„ skrypty przed wyciÄ…gniÄ™ciem tekstu
            body_copy = BeautifulSoup(str(body), "html.parser")
            for script in body_copy.find_all("script"):
                script.decompose()
            for style in body_copy.find_all("style"):
                style.decompose()

            body_text = clean_text(body_copy.get_text()).lower()
            # JeÅ›li gÅ‚Ã³wna widoczna treÅ›Ä‡ strony jest bardzo krÃ³tka i zawiera specyficzne bÅ‚Ä™dy 404, to prawdopodobnie 404
            error_patterns_in_body = [
                "404 error", "404 not found", "page not found", "error 404",
                "http 404", "404 - not found", "the page you are looking for",
                "this page doesn't exist"
            ]
            if len(body_text) < 500 and any(pattern in body_text for pattern in error_patterns_in_body):
                return True

    # SprawdÅº czy strona nie zawiera podstawowych elementÃ³w strony perfum
    # (np. brak nazwy perfum, opisu itp.)
    if not soup.find("h1", itemprop="name") and not soup.find(id="pyramid"):
        # JeÅ›li strona nie zawiera podstawowych elementÃ³w perfum
        # i jest krÃ³tka, prawdopodobnie to bÅ‚Ä…d
        if len(html) < 5000:
            return True

    return False


def remove_unwanted_elements(soup: BeautifulSoup) -> None:
    """Usuwa wszystkie skrypty, iframy i SVG z HTML."""
    # UsuÅ„ wszystkie skrypty
    for script in soup.find_all("script"):
        script.decompose()
    
    # UsuÅ„ wszystkie iframy
    for iframe in soup.find_all("iframe"):
        iframe.decompose()
    
    # UsuÅ„ wszystkie elementy SVG
    for svg in soup.find_all("svg"):
        svg.decompose()


def extract_perfume_name(soup: BeautifulSoup) -> str:
    """WyciÄ…ga nazwÄ™ perfum."""
    # SprÃ³buj znaleÅºÄ‡ w h1 z itemprop="name"
    h1 = soup.find("h1", itemprop="name")
    if h1:
        # UsuÅ„ small tag jeÅ›li istnieje
        small = h1.find("small")
        if small:
            small.decompose()
        return clean_text(h1.get_text())
    
    # Alternatywnie w tytule strony
    title = soup.find("title")
    if title:
        return clean_text(title.get_text())
    
    return ""


def extract_brand(soup: BeautifulSoup) -> Optional[str]:
    """WyciÄ…ga markÄ™ perfum."""
    # SprÃ³buj znaleÅºÄ‡ w elemencie z itemprop="brand"
    brand_elem = soup.find(itemprop="brand")
    if brand_elem:
        brand_name = brand_elem.find(itemprop="name")
        if brand_name:
            return clean_text(brand_name.get_text())
        return clean_text(brand_elem.get_text())
    
    return None


def extract_description(soup: BeautifulSoup) -> str:
    """WyciÄ…ga opis perfum."""
    description = ""
    
    # SprÃ³buj znaleÅºÄ‡ w elemencie z itemprop="description"
    desc_elem = soup.find(itemprop="description")
    if desc_elem:
        description = clean_text(desc_elem.get_text())
    
    # Alternatywnie szukaj w meta description
    if not description:
        meta_desc = soup.find("meta", attrs={"name": "description"})
        if meta_desc and meta_desc.get("content"):
            description = clean_text(meta_desc.get("content"))
    
    # JeÅ›li znaleziono "Read about this perfume", usuÅ„ wszystko od tego miejsca do koÅ„ca
    if description and "Read about this perfume" in description:
        index = description.find("Read about this perfume")
        description = description[:index].strip()
    
    return description


def extract_main_image_url(soup: BeautifulSoup, base_url: str) -> str:
    """WyciÄ…ga URL gÅ‚Ã³wnego obrazu."""
    # SprÃ³buj znaleÅºÄ‡ obraz z itemprop="image"
    img = soup.find("img", itemprop="image")
    if img:
        src = img.get("src") or img.get("data-src")
        if src:
            return urljoin(base_url, src)
    
    # Alternatywnie znajdÅº pierwszy obraz w sekcji gÅ‚Ã³wnej
    picture = soup.find("picture")
    if picture:
        img = picture.find("img")
        if img:
            src = img.get("src") or img.get("data-src")
            if src:
                return urljoin(base_url, src)
    
    return ""


def extract_rating(soup: BeautifulSoup) -> Optional[float]:
    """WyciÄ…ga ocenÄ™ perfum."""
    # SprÃ³buj znaleÅºÄ‡ w elemencie z itemprop="ratingValue"
    rating_elem = soup.find(itemprop="ratingValue")
    if rating_elem:
        try:
            return float(clean_text(rating_elem.get_text()))
        except ValueError:
            pass
    
    # Szukaj w rÃ³Å¼nych formatach oceny
    rating_patterns = [
        r"rating[\"']?\s*[:=]\s*([\d.]+)",
        r"(\d+\.\d+)\s*/\s*\d+",
        r"(\d+)\s*z\s*\d+",
    ]
    text = soup.get_text()
    for pattern in rating_patterns:
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            try:
                return float(match.group(1))
            except ValueError:
                pass
    
    return None


def extract_rating_count(soup: BeautifulSoup) -> Optional[int]:
    """WyciÄ…ga liczbÄ™ ocen."""
    # SprÃ³buj znaleÅºÄ‡ w elemencie z itemprop="ratingCount" lub "reviewCount"
    count_elem = soup.find(itemprop=lambda x: x in ["ratingCount", "reviewCount"])
    if count_elem:
        try:
            # Najpierw sprawdÅº atrybut content (bardziej niezawodny)
            content = count_elem.get("content")
            if content:
                return int(content)
            
            # JeÅ›li nie ma content, sprÃ³buj wyciÄ…gnÄ…Ä‡ z tekstu
            text = clean_text(count_elem.get_text())
            # WyciÄ…gnij liczby z tekstu (usuÅ„ przecinki/separatory)
            numbers = re.findall(r"\d+", text.replace(",", "").replace(".", ""))
            if numbers:
                return int(numbers[0])
        except (ValueError, AttributeError):
            pass
    
    return None


def extract_user_reviews(soup: BeautifulSoup) -> List[str]:
    """WyciÄ…ga recenzje uÅ¼ytkownikÃ³w."""
    reviews = []
    
    # Szukaj elementÃ³w z itemprop="review"
    review_elems = soup.find_all(itemprop="review")
    
    for review_elem in review_elems:
        # ZnajdÅº tekst recenzji - zwykle w elemencie z itemprop="reviewBody" lub w div z klasÄ…
        review_body = review_elem.find(itemprop="reviewBody")
        if review_body:
            text = clean_text(review_body.get_text())
        else:
            # SprÃ³buj znaleÅºÄ‡ gÅ‚Ã³wny tekst recenzji (pomiÅ„ ratingi i metadane)
            # Szukaj w elementach z klasÄ… fragrance-review-box
            review_box = review_elem.find(class_=re.compile(r"review", re.I))
            if review_box:
                text = clean_text(review_box.get_text())
            else:
                # Pobierz caÅ‚y tekst ale usuÅ„ ratingi
                text = clean_text(review_elem.get_text())
        
        # Filtruj recenzje (minimalna dÅ‚ugoÅ›Ä‡, nie tylko ratingi)
        if text and len(text) > 30:  # Minimum 30 znakÃ³w dla znaczÄ…cej recenzji
            # UsuÅ„ duplikaty
            if text not in reviews:
                reviews.append(text)
    
    return reviews[:50]  # Limit do 50 recenzji


def extract_notes(soup: BeautifulSoup) -> Dict[str, List[str]]:
    """WyciÄ…ga nuty zapachowe (top, heart, base)."""
    notes = {
        "topNotes": [],
        "heartNotes": [],
        "baseNotes": [],
    }
    
    # ZnajdÅº sekcjÄ™ pyramid
    pyramid = soup.find(id="pyramid")
    if not pyramid:
        return notes
    
    # Mapowanie polskich nazw na klucze
    category_map = {
        "Nuty gÅ‚owy": "topNotes",
        "Top notes": "topNotes",
        "Nuty serca": "heartNotes",
        "Heart notes": "heartNotes",
        "Middle notes": "heartNotes",
        "Nuty bazy": "baseNotes",
        "Base notes": "baseNotes",
    }
    
    # ZnajdÅº wszystkie nagÅ‚Ã³wki h4 w sekcji pyramid
    h4_tags = pyramid.find_all("h4")
    current_category = None
    
    for h4 in h4_tags:
        h4_text = clean_text(h4.get_text())
        for key, category in category_map.items():
            if key.lower() in h4_text.lower():
                current_category = category
                break
        
        if current_category:
            # ZnajdÅº wszystkie linki w nastÄ™pnym kontenerze po h4
            # Szukaj w nastÄ™pnych siblingach lub w kontenerach div
            container = h4.find_next_sibling()
            if not container:
                # SprÃ³buj znaleÅºÄ‡ kontener w rodzicu
                parent = h4.find_parent()
                if parent:
                    # Szukaj w nastÄ™pnych elementach w tym samym kontenerze
                    for sibling in parent.find_next_siblings(limit=3):
                        if sibling.name == "div":
                            container = sibling
                            break
            
            if container:
                # Szukaj linkÃ³w z nutami zapachowymi (typowo sÄ… w div z flexbox)
                links = container.find_all("a", href=re.compile(r"/nuty|/notes", re.I))
                
                for link in links:
                    # Tekst nuty jest czÄ™sto po linku (jako tekst w rodzicu)
                    parent_div = link.find_parent("div")
                    if parent_div:
                        # Pobierz caÅ‚y tekst z div, ale usuÅ„ tekst z linka (link-span)
                        link_text = link.get_text()
                        parent_text = clean_text(parent_div.get_text())
                        # UsuÅ„ tekst z linka z tekstu rodzica
                        note_text = parent_text.replace(link_text, "").strip()
                        
                        # JeÅ›li nie ma tekstu, sprÃ³buj pobraÄ‡ bezpoÅ›rednio z linka
                        if not note_text:
                            # UsuÅ„ link-span z linka
                            link_copy = BeautifulSoup(str(link), "html.parser")
                            for span in link_copy.find_all(class_="link-span"):
                                span.decompose()
                            note_text = clean_text(link_copy.get_text())
                        
                        # PomiÅ„ puste i krÃ³tkie teksty
                        if note_text and len(note_text) > 1 and note_text not in notes[current_category]:
                            notes[current_category].append(note_text)
                
                # JeÅ›li nie znaleziono linkÃ³w, szukaj tekstu bezpoÅ›rednio w kontenerze
                if not links:
                    # Szukaj w div z nutami (czasem sÄ… wyÅ›wietlane jako tekst)
                    text_containers = container.find_all("div", recursive=True)
                    for text_container in text_containers:
                        text = clean_text(text_container.get_text())
                        if text and len(text) > 2 and len(text) < 100 and text not in notes[current_category]:
                            # SprawdÅº czy to wyglÄ…da na nazwÄ™ nuty (nie za dÅ‚ugi, nie za krÃ³tki)
                            if 2 < len(text) < 50:
                                notes[current_category].append(text)
    
    return notes


def extract_similar_perfumes(soup: BeautifulSoup) -> List[Dict[str, str]]:
    """WyciÄ…ga podobne perfumy."""
    similar = []
    
    # Szukaj sekcji z podobnymi perfumami - typowe nazwy
    similar_keywords = ["podobne", "similar", "this reminds", "reminds me"]
    
    for keyword in similar_keywords:
        # Szukaj w nagÅ‚Ã³wkach, linkach, divach
        sections = soup.find_all(
            lambda tag: tag.name in ["h2", "h3", "h4", "div", "section"]
            and keyword.lower() in clean_text(tag.get_text()).lower()
        )
        
        for section in sections:
            # ZnajdÅº wszystkie linki w sekcji lub w nastÄ™pnych elementach
            container = section.find_next_sibling() or section.parent
            if container:
                links = container.find_all("a", href=re.compile(r"/perfume|/perfumes", re.I))
                for link in links:
                    name = clean_text(link.get_text())
                    if name and len(name) > 2 and name not in [s["name"] for s in similar]:
                        similar.append({"name": name})
    
    return similar[1:13]  


def extract_people_also_like(soup: BeautifulSoup) -> List[Dict[str, str]]:
    """WyciÄ…ga perfumy z sekcji 'People who like this also like'."""
    perfumes = []
    
    # ZnajdÅº span z tekstem "People who like this also like"
    title_span = soup.find("span", string=re.compile(r"People who like this also like", re.I))
    if not title_span:
        # Alternatywnie szukaj w span ktÃ³ry zawiera ten tekst
        title_span = soup.find("span", string=lambda text: text and "People who like this also like" in text)
    
    if not title_span:
        return perfumes
    
    # ZnajdÅº kontener strike-title (rodzic span)
    strike_title = title_span.find_parent(class_="strike-title")
    if not strike_title:
        # JeÅ›li nie ma klasy strike-title, uÅ¼yj bezpoÅ›redniego rodzica
        strike_title = title_span.find_parent()
    
    if not strike_title:
        return perfumes
    
    # ZnajdÅº nastÄ™pny element carousel (moÅ¼e byÄ‡ nastÄ™pnym siblingem lub w nastÄ™pnym div)
    carousel = strike_title.find_next_sibling(class_=re.compile(r"carousel", re.I))
    if not carousel:
        # Szukaj w nastÄ™pnych siblingach
        current = strike_title.next_sibling
        while current:
            if hasattr(current, 'get') and isinstance(current, Tag):
                if 'carousel' in current.get('class', []):
                    carousel = current
                    break
            current = current.next_sibling if hasattr(current, 'next_sibling') else None
    
    if not carousel:
        # SprÃ³buj znaleÅºÄ‡ carousel w rodzicu
        parent = strike_title.find_parent()
        if parent:
            carousel = parent.find(class_=re.compile(r"carousel", re.I))
    
    if not carousel:
        return perfumes
    
    # ZnajdÅº wszystkie carousel-cell w carousel
    cells = carousel.find_all(class_="carousel-cell")
    
    for cell in cells:
        # ZnajdÅº link w cell
        link = cell.find("a", href=re.compile(r"/perfume", re.I))
        if not link:
            continue
        
        # WyciÄ…gnij markÄ™
        brand_span = link.find("span", class_="brand")
        brand = ""
        if brand_span:
            brand = clean_text(brand_span.get_text())
        
        # WyciÄ…gnij nazwÄ™ perfum (w span z klasÄ… ztworowseclipse lub jako tekst linka)
        name_span = link.find("span", class_="ztworowseclipse")
        name = ""
        if name_span:
            name = clean_text(name_span.get_text())
        else:
            # JeÅ›li nie ma span z nazwÄ…, sprÃ³buj wyciÄ…gnÄ…Ä‡ z caÅ‚ego tekstu linka
            link_text = clean_text(link.get_text())
            # UsuÅ„ markÄ™ z tekstu
            if brand:
                link_text = link_text.replace(brand, "").strip()
            name = link_text
        
        # JeÅ›li nadal nie ma nazwy, sprÃ³buj wyciÄ…gnÄ…Ä‡ z href lub alt obrazu
        if not name:
            img = link.find("img")
            if img and img.get("alt"):
                alt_text = img.get("alt")
                # UsuÅ„ markÄ™ z alt jeÅ›li jest
                if brand and brand in alt_text:
                    name = alt_text.replace(brand, "").strip()
                else:
                    name = alt_text
        
        # Normalizuj nazwÄ™ i markÄ™
        name = clean_text(name)
        brand = clean_text(brand)
        
        # Dodaj tylko jeÅ›li mamy nazwÄ™
        if name and len(name) > 1:
            # SprawdÅº duplikaty (normalizujÄ…c nazwy)
            name_normalized = name.lower().strip()
            brand_normalized = brand.lower().strip() if brand else ""
            
            is_duplicate = False
            for existing in perfumes:
                existing_name = existing.get("name", "").lower().strip()
                existing_brand = existing.get("brand", "").lower().strip()
                if (name_normalized == existing_name and 
                    (not brand_normalized or not existing_brand or brand_normalized == existing_brand)):
                    is_duplicate = True
                    break
            
            if not is_duplicate:
                perfume_data = {"name": name}
                if brand:
                    perfume_data["brand"] = brand
                perfumes.append(perfume_data)
    
    return perfumes


def extract_reminds_me_perfumes(soup: BeautifulSoup) -> List[Dict[str, str]]:
    """WyciÄ…ga perfumy z sekcji 'This perfume reminds me of'."""
    perfumes = []
    
    # Szukaj span z tekstem "reminds me" lub podobnym (podobnie jak w extract_people_also_like)
    reminds_keywords = [
        "this perfume reminds me",
        "reminds me of",
        "this reminds me",
        "reminds me",
        "przypomina mi",
        "to perfum przypomina mi",
    ]
    
    title_span = None
    for keyword in reminds_keywords:
        # Najpierw szukaj span z dokÅ‚adnym tekstem
        title_span = soup.find("span", string=re.compile(keyword, re.I))
        if not title_span:
            # Alternatywnie szukaj w span ktÃ³ry zawiera ten tekst
            title_span = soup.find("span", string=lambda text: text and keyword.lower() in text.lower())
        if title_span:
            break
    
    # JeÅ›li nie znaleziono span, szukaj w div
    if not title_span:
        for keyword in reminds_keywords:
            title_div = soup.find(
                lambda tag: tag.name == "div"
                and keyword.lower() in clean_text(tag.get_text()).lower()
            )
            if title_div:
                # ZnajdÅº span w tym div lub uÅ¼yj div jako punktu odniesienia
                title_span = title_div.find("span")
                if not title_span:
                    # UÅ¼yj div jako punktu odniesienia
                    title_span = title_div
                break
    
    if not title_span:
        return perfumes
    
    # ZnajdÅº kontener strike-title (rodzic span)
    strike_title = title_span.find_parent(class_="strike-title")
    if not strike_title:
        # JeÅ›li nie ma klasy strike-title, uÅ¼yj bezpoÅ›redniego rodzica
        strike_title = title_span.find_parent()
    
    if not strike_title:
        return perfumes
    
    # ZnajdÅº nastÄ™pny element carousel (moÅ¼e byÄ‡ nastÄ™pnym siblingem lub w nastÄ™pnym div)
    carousel = strike_title.find_next_sibling(class_=re.compile(r"carousel", re.I))
    if not carousel:
        # Szukaj w nastÄ™pnych siblingach
        current = strike_title.next_sibling
        while current:
            if hasattr(current, 'get') and isinstance(current, Tag):
                if 'carousel' in current.get('class', []):
                    carousel = current
                    break
            current = current.next_sibling if hasattr(current, 'next_sibling') else None
    
    if not carousel:
        # SprÃ³buj znaleÅºÄ‡ carousel w rodzicu
        parent = strike_title.find_parent()
        if parent:
            carousel = parent.find(class_=re.compile(r"carousel", re.I))
    
    if not carousel:
        # Szukaj flickity-slider (typowy kontener dla carousel)
        flickity = strike_title.find_next_sibling(class_="flickity-slider")
        if not flickity:
            flickity = strike_title.find_parent(class_="flickity-slider")
        if flickity:
            carousel = flickity
    
    # JeÅ›li nadal nie znaleziono, szukaj flickity-slider w nastÄ™pnych elementach po strike_title
    if not carousel:
        # Szukaj w nastÄ™pnych siblingach strike_title
        current = strike_title.next_sibling
        while current:
            if hasattr(current, 'get') and isinstance(current, Tag):
                if 'flickity-slider' in current.get('class', []):
                    carousel = current
                    break
            current = current.next_sibling if hasattr(current, 'next_sibling') else None
    
    # JeÅ›li nadal nie znaleziono, szukaj wszystkich flickity-slider i sprawdÅº czy ktÃ³ryÅ› jest w sekcji z "reminds me"
    if not carousel:
        all_flickity = soup.find_all(class_="flickity-slider")
        for flickity in all_flickity:
            # SprawdÅº czy w okolicy tego flickity-slider jest tekst "reminds me"
            parent = flickity.find_parent()
            if parent:
                parent_text = clean_text(parent.get_text()).lower()
                if any(keyword in parent_text for keyword in ["reminds me", "this perfume reminds", "przypomina mi", "to perfum przypomina"]):
                    carousel = flickity
                    break
    
    if not carousel:
        return perfumes
    
    # ZnajdÅº wszystkie carousel-cell w carousel
    cells = carousel.find_all(class_="carousel-cell")
    
    for cell in cells:
        # ZnajdÅº link w cell
        link = cell.find("a", href=re.compile(r"/perfume", re.I))
        if not link:
            continue
        
        # WyciÄ…gnij brand i name z href (format: /perfume/Brand/Name-ID.html)
        href = link.get("href", "")
        brand = ""
        name = ""
        
        if href:
            # Parsuj href: /perfume/Brand/Name-ID.html
            # Nazwa moÅ¼e zawieraÄ‡ myÅ›lniki, wiÄ™c szukamy wszystkiego przed ostatnim -ID.html
            match = re.match(r"/perfume/([^/]+)/(.+?)-(\d+)\.html", href)
            if match:
                brand = match.group(1).replace("-", " ")
                name = match.group(2).replace("-", " ")
        
        # JeÅ›li nie udaÅ‚o siÄ™ wyciÄ…gnÄ…Ä‡ z href, sprÃ³buj z alt/title obrazu
        if not name or not brand:
            img = link.find("img")
            if img:
                alt_text = img.get("alt", "") or img.get("title", "")
                if alt_text:
                    # Format: "Name Brand" lub "Brand Name"
                    parts = alt_text.split()
                    if len(parts) >= 2:
                        # CzÄ™sto ostatnie sÅ‚owa to brand, reszta to name
                        # SprÃ³buj znaleÅºÄ‡ brand w ostatnich 2-3 sÅ‚owach
                        if not brand:
                            # SprawdÅº czy ktÃ³ryÅ› z ostatnich elementÃ³w wyglÄ…da na brand
                            for i in range(max(1, len(parts) - 2), len(parts)):
                                potential_brand = " ".join(parts[i:])
                                if len(potential_brand) > 2:
                                    brand = potential_brand
                                    name = " ".join(parts[:i])
                                    break
                        if not name:
                            name = alt_text.replace(brand, "").strip() if brand else alt_text
        
        # JeÅ›li nadal nie ma nazwy, sprÃ³buj wyciÄ…gnÄ…Ä‡ z tekstu linka
        if not name:
            link_text = clean_text(link.get_text())
            if link_text and len(link_text) > 2:
                name = link_text
                # UsuÅ„ markÄ™ z nazwy jeÅ›li jest
                if brand and brand in name:
                    name = name.replace(brand, "").strip()
        
        # Normalizuj nazwÄ™ i markÄ™
        name = clean_text(name)
        brand = clean_text(brand)
        
        # Dodaj tylko jeÅ›li mamy nazwÄ™
        if name and len(name) > 1:
            # SprawdÅº duplikaty (normalizujÄ…c nazwy)
            name_normalized = name.lower().strip()
            brand_normalized = brand.lower().strip() if brand else ""
            
            is_duplicate = False
            for existing in perfumes:
                existing_name = existing.get("name", "").lower().strip()
                existing_brand = existing.get("brand", "").lower().strip()
                if (name_normalized == existing_name and 
                    (not brand_normalized or not existing_brand or brand_normalized == existing_brand)):
                    is_duplicate = True
                    break
            
            if not is_duplicate:
                perfume_data = {"name": name}
                if brand:
                    perfume_data["brand"] = brand
                perfumes.append(perfume_data)
    
    return perfumes


def extract_recommended_perfumes(soup: BeautifulSoup) -> List[Dict[str, str]]:
    """WyciÄ…ga rekomendowane perfumy z sekcji 'People who like this also like'."""
    # UÅ¼yj dedykowanej funkcji dla "People who like this also like"
    perfumes = extract_people_also_like(soup)
    
    # JeÅ›li nie znaleziono, sprÃ³buj alternatywnych metod
    if not perfumes:
        # Szukaj sekcji z rekomendacjami - typowe nazwy
        rec_keywords = ["rekomendowane", "recommended", "suggested", "you may also like"]
        
        for keyword in rec_keywords:
            # Szukaj w nagÅ‚Ã³wkach, linkach, divach
            sections = soup.find_all(
                lambda tag: tag.name in ["h2", "h3", "h4", "div", "section"]
                and keyword.lower() in clean_text(tag.get_text()).lower()
            )
            
            for section in sections:
                # ZnajdÅº wszystkie linki w sekcji lub w nastÄ™pnych elementach
                container = section.find_next_sibling() or section.parent
                if container:
                    links = container.find_all("a", href=re.compile(r"/perfume|/perfumes", re.I))
                    for link in links:
                        name = clean_text(link.get_text())
                        if name and len(name) > 2:
                            # SprawdÅº duplikaty
                            if name not in [r["name"] for r in perfumes]:
                                perfumes.append({"name": name})
    
    return perfumes[:20]  # Limit do 20 rekomendowanych


def extract_pros(soup: BeautifulSoup) -> List[str]:
    """WyciÄ…ga pros (zalety) z sekcji Pros w HTML."""
    pros_list = []
    
    # ZnajdÅº gÅ‚Ã³wny div z Pros (z klasÄ… cell small-12 medium-6)
    # Szukamy diva ktÃ³ry zawiera tekst "Pros" w nagÅ‚Ã³wku
    pros_section = None
    for div in soup.find_all('div', class_='cell small-12 medium-6'):
        header = div.find('h4', class_='header')
        if header and 'Pros' in clean_text(header.get_text()):
            pros_section = div
            break
    
    if not pros_section:
        return pros_list
    
    # ZnajdÅº wszystkie span wewnÄ…trz divÃ³w z klasÄ… "cell small-12"
    # ktÃ³re sÄ… wewnÄ…trz sekcji pros (pomijajÄ…c spany z liczbami w num-votes-sp)
    for item_div in pros_section.find_all('div', class_='cell small-12'):
        # ZnajdÅº span ktÃ³ry NIE jest wewnÄ…trz num-votes-sp
        all_spans = item_div.find_all('span')
        for span in all_spans:
            # SprawdÅº czy span nie jest wewnÄ…trz num-votes-sp
            parent_num_votes = span.find_parent('div', class_='num-votes-sp')
            if not parent_num_votes:
                text = clean_text(span.get_text())
                # Upewnij siÄ™ Å¼e to nie jest liczba (czyli pros tekst)
                if text and not text.isdigit():
                    pros_list.append(text)
                    break  # WeÅº tylko pierwszy span ktÃ³ry nie jest liczbÄ…
    
    # ZwrÃ³Ä‡ tylko pierwsze 5 pros
    return pros_list[:5]


def extract_cons(soup: BeautifulSoup) -> List[str]:
    """WyciÄ…ga cons (wady) z sekcji Cons w HTML."""
    cons_list = []
    
    # ZnajdÅº gÅ‚Ã³wny div z Cons (z klasÄ… cell small-12 medium-6)
    # Szukamy diva ktÃ³ry zawiera tekst "Cons" w nagÅ‚Ã³wku
    cons_section = None
    for div in soup.find_all('div', class_='cell small-12 medium-6'):
        header = div.find('h4', class_='header')
        if header and 'Cons' in clean_text(header.get_text()):
            cons_section = div
            break
    
    if not cons_section:
        return cons_list
    
    # ZnajdÅº wszystkie span wewnÄ…trz divÃ³w z klasÄ… "cell small-12"
    # ktÃ³re sÄ… wewnÄ…trz sekcji cons (pomijajÄ…c spany z liczbami w num-votes-sp)
    for item_div in cons_section.find_all('div', class_='cell small-12'):
        # ZnajdÅº span ktÃ³ry NIE jest wewnÄ…trz num-votes-sp
        all_spans = item_div.find_all('span')
        for span in all_spans:
            # SprawdÅº czy span nie jest wewnÄ…trz num-votes-sp
            parent_num_votes = span.find_parent('div', class_='num-votes-sp')
            if not parent_num_votes:
                text = clean_text(span.get_text())
                # Upewnij siÄ™ Å¼e to nie jest liczba (czyli cons tekst)
                if text and not text.isdigit():
                    cons_list.append(text)
                    break  # WeÅº tylko pierwszy span ktÃ³ry nie jest liczbÄ…
    
    # ZwrÃ³Ä‡ tylko pierwsze 5 cons
    return cons_list[:5]


def extract_voting_data(soup: BeautifulSoup, category: str, options_mapping: Dict[str, List[str]]) -> Dict[str, Any]:
    """WyciÄ…ga dane gÅ‚osowania dla danej kategorii.
    
    options_mapping: Dict z angielskÄ… nazwÄ… opcji jako kluczem i listÄ… polskich wariantÃ³w jako wartoÅ›ciÄ…
    """
    data = {}
    most_voted_value = None
    max_votes = 0
    
    # Mapowanie nazw kategorii na tytuÅ‚y w HTML
    category_titles = {
        "longevity": ["LONGEVITY"],
        "gender": ["GENDER", "PÅEÄ†"],
        "valueForMoney": ["VALUE FOR MONEY", "STOSUNEK JAKOÅšÄ†/CENA"],
        "season": ["SEASON", "PORA ROKU"],
        "timeOfDay": ["TIME OF DAY", "PORA DNIA"],
        "sillage": ["SILLAGE"],
    }
    
    # ZnajdÅº sekcjÄ™ kategorii po tytule
    category_section = None
    titles = category_titles.get(category, [category.upper()])
    
    for title in titles:
        # Szukaj span lub innego elementu z tekstem tytuÅ‚u
        title_elem = soup.find(lambda tag: tag.name in ["span", "h2", "h3", "h4", "div"] 
                               and clean_text(tag.get_text()).upper() == title.upper())
        if title_elem:
            # ZnajdÅº kontener sekcji (zwykle rodzic lub dziadek)
            category_section = title_elem.find_parent(class_=re.compile(r"cell|section|container", re.I))
            if not category_section:
                category_section = title_elem.find_parent("div")
            if category_section:
                break
    
    # JeÅ›li nie znaleziono sekcji, szukaj w caÅ‚ym soup
    search_soup = category_section if category_section else soup
    
    # ZnajdÅº wszystkie elementy vote-button-name w sekcji
    vote_names = search_soup.find_all(class_="vote-button-name")
    
    for vote_name_elem in vote_names:
        vote_name_text = clean_text(vote_name_elem.get_text()).lower()
        
        # ZnajdÅº kontener grid-x ktÃ³ry zawiera ten element
        grid_container = vote_name_elem.find_parent(class_=re.compile(r"grid", re.I))
        if not grid_container:
            grid_container = vote_name_elem.find_parent()
        
        # ZnajdÅº odpowiadajÄ…cy element z liczbÄ… gÅ‚osÃ³w w tym samym kontenerze
        if grid_container:
            vote_legend = grid_container.find(class_="vote-button-legend")
            if vote_legend:
                vote_count_text = clean_text(vote_legend.get_text())
                numbers = re.findall(r"\d+", vote_count_text)
                if numbers:
                    vote_count = int(numbers[0])
                    
                    # SprawdÅº, ktÃ³ra opcja pasuje
                    # Strategia: najpierw dokÅ‚adne dopasowania, potem czÄ™Å›ciowe (najdÅ‚uÅ¼sze najpierw)
                    matched = False
                    vote_name_lower = vote_name_text.lower()
                    vote_name_normalized = vote_name_lower.replace(" ", "")
                    
                    # KROK 1: SprawdÅº dokÅ‚adne dopasowania (po normalizacji spacji)
                    for eng_option, variants in options_mapping.items():
                        for variant in variants:
                            variant_normalized = variant.lower().replace(" ", "")
                            if variant_normalized == vote_name_normalized:
                                data[eng_option] = vote_count
                                if vote_count > max_votes:
                                    max_votes = vote_count
                                    most_voted_value = eng_option
                                matched = True
                                break
                        if matched:
                            break
                    
                    # KROK 2: JeÅ›li nie znaleziono dokÅ‚adnego, sprawdÅº czÄ™Å›ciowe dopasowania
                    # Sortuj opcje od najdÅ‚uÅ¼szych wariantÃ³w do najkrÃ³tszych (Å¼eby "more female" pasowaÅ‚o przed "female")
                    if not matched:
                        sorted_options = sorted(
                            options_mapping.items(),
                            key=lambda x: max(len(v.replace(" ", "")) for v in x[1]),
                            reverse=True
                        )
                        for eng_option, variants in sorted_options:
                            # Sortuj warianty od najdÅ‚uÅ¼szych do najkrÃ³tszych
                            sorted_variants = sorted(variants, key=lambda v: len(v.replace(" ", "")), reverse=True)
                            for variant in sorted_variants:
                                variant_lower = variant.lower()
                                # SprawdÅº czy wariant jest zawarty w tekÅ›cie (ale nie na odwrÃ³t!)
                                # To zapobiega dopasowaniu "kobieta" do "kobieta / unisex"
                                if variant_lower in vote_name_lower:
                                    data[eng_option] = vote_count
                                    if vote_count > max_votes:
                                        max_votes = vote_count
                                        most_voted_value = eng_option
                                    matched = True
                                    break
                            if matched:
                                break
    
    if most_voted_value and max_votes > 0:
        data["mostVoted"] = most_voted_value
    
    return data


def extract_percentage_width_data(soup: BeautifulSoup, category: str, options_mapping: Dict[str, List[str]]) -> Dict[str, Any]:
    """WyciÄ…ga wartoÅ›ci procentowe width dla danej kategorii (season, timeOfDay).
    
    Szuka elementÃ³w z vote-button-legend i odpowiadajÄ…cych im wartoÅ›ci width w stylach.
    
    options_mapping: Dict z angielskÄ… nazwÄ… opcji jako kluczem i listÄ… wariantÃ³w jako wartoÅ›ciÄ…
    """
    data = {}
    most_voted_value = None
    max_percentage = 0.0
    
    # Mapowanie nazw kategorii na tytuÅ‚y w HTML
    category_titles = {
        "season": ["SEASON", "PORA ROKU"],
        "timeOfDay": ["TIME OF DAY", "PORA DNIA"],
    }
    
    # ZnajdÅº sekcjÄ™ kategorii po tytule
    category_section = None
    titles = category_titles.get(category, [category.upper()])
    
    for title in titles:
        # Szukaj span lub innego elementu z tekstem tytuÅ‚u
        title_elem = soup.find(lambda tag: tag.name in ["span", "h2", "h3", "h4", "div"] 
                               and clean_text(tag.get_text()).upper() == title.upper())
        if title_elem:
            # ZnajdÅº kontener sekcji (zwykle rodzic lub dziadek)
            category_section = title_elem.find_parent(class_=re.compile(r"cell|section|container", re.I))
            if not category_section:
                category_section = title_elem.find_parent("div")
            if category_section:
                break
    
    # JeÅ›li nie znaleziono sekcji, szukaj w caÅ‚ym soup
    search_soup = category_section if category_section else soup
    
    # ZnajdÅº wszystkie elementy vote-button-legend w sekcji
    vote_legends = search_soup.find_all(class_="vote-button-legend")
    
    for vote_legend in vote_legends:
        legend_text = clean_text(vote_legend.get_text()).lower()
        
        # ZnajdÅº kontener ktÃ³ry zawiera ten element (szukaj rodzica z index)
        container = vote_legend.find_parent(attrs={"index": True})
        if not container:
            # JeÅ›li nie ma index, szukaj w rodzicu
            container = vote_legend.find_parent()
        
        if container:
            # ZnajdÅº div z voting-small-chart-size w tym kontenerze
            chart_div = container.find("div", class_="voting-small-chart-size")
            if chart_div:
                # ZnajdÅº wszystkie divy z stylem
                inner_divs = chart_div.find_all("div", style=True)
                for div in inner_divs:
                    style = div.get("style", "")
                    # Szukamy diva z background rgb (nie rgba) - to jest wewnÄ™trzny div z width
                    if "background:" in style and "rgb(" in style and "rgba(" not in style:
                        # WyodrÄ™bnij width z stylu
                        width_match = re.search(r"width:\s*([\d.]+)%", style)
                        if width_match:
                            width_percent = float(width_match.group(1))
                            
                            # SprawdÅº, ktÃ³ra opcja pasuje
                            matched = False
                            legend_lower = legend_text.lower()
                            legend_normalized = legend_lower.replace(" ", "")
                            
                            # KROK 1: SprawdÅº dokÅ‚adne dopasowania
                            for eng_option, variants in options_mapping.items():
                                for variant in variants:
                                    variant_normalized = variant.lower().replace(" ", "")
                                    if variant_normalized == legend_normalized:
                                        data[eng_option] = width_percent
                                        if width_percent > max_percentage:
                                            max_percentage = width_percent
                                            most_voted_value = eng_option
                                        matched = True
                                        break
                                if matched:
                                    break
                            
                            # KROK 2: JeÅ›li nie znaleziono dokÅ‚adnego, sprawdÅº czÄ™Å›ciowe dopasowania
                            if not matched:
                                sorted_options = sorted(
                                    options_mapping.items(),
                                    key=lambda x: max(len(v.replace(" ", "")) for v in x[1]),
                                    reverse=True
                                )
                                for eng_option, variants in sorted_options:
                                    sorted_variants = sorted(variants, key=lambda v: len(v.replace(" ", "")), reverse=True)
                                    for variant in sorted_variants:
                                        variant_lower = variant.lower()
                                        if variant_lower in legend_lower:
                                            data[eng_option] = width_percent
                                            if width_percent > max_percentage:
                                                max_percentage = width_percent
                                                most_voted_value = eng_option
                                            matched = True
                                            break
                                    if matched:
                                        break
                            break
    
    if most_voted_value and max_percentage > 0:
        data["mostVoted"] = most_voted_value
    
    return data


def extract_all_voting_data(soup: BeautifulSoup) -> Dict[str, Dict[str, Any]]:
    """WyciÄ…ga wszystkie dane gÅ‚osowania."""
    return {
        "longevity": extract_voting_data(
            soup,
            "longevity",
            {
                "veryWeak": ["very weak", "bardzo sÅ‚aba"],
                "weak": ["weak", "sÅ‚aba"],
                "moderate": ["moderate", "przeciÄ™tna"],
                "longLasting": ["long lasting", "dÅ‚ugotrwaÅ‚a"],
                "eternal": ["eternal", "wieczna"],
            },
        ),
        "gender": extract_voting_data(
            soup,
            "gender",
            {
                # WaÅ¼ne: kolejnoÅ›Ä‡ i dÅ‚ugoÅ›Ä‡ wariantÃ³w jest istotna - dÅ‚uÅ¼sze/more specyficzne najpierw
                "moreFemale": ["more female", "morefemale", "more feminine"],
                "female": ["female", "kobieta", "feminine", "woman", "women", "kobiet", "for women"],
                "unisex": ["unisex", "uni-sex"],
                "moreMale": ["more male", "moremale", "more masculine"],
                "male": ["male", "mÄ™Å¼czyzna", "masculine", "man", "men", "mÄ™Å¼czyzn", "for men"],
            },
        ),
        "valueForMoney": extract_voting_data(
            soup,
            "valueForMoney",
            {
                "priceTooHigh": ["way overpriced", "cena za wysoka", "price too high"],
                "overpriced": ["overpriced", "zawyÅ¼ona cena"],
                "fair": ["ok", "fair"],
                "goodQuality": ["good value", "dobra jakoÅ›Ä‡", "good quality"],
                "excellentQuality": ["great value", "doskonaÅ‚a jakoÅ›Ä‡", "excellent quality"],
            },
        ),
        "season": extract_percentage_width_data(
            soup,
            "season",
            {
                "winter": ["winter", "zima"],
                "spring": ["spring", "wiosna"],
                "summer": ["summer", "lato"],
                "fall": ["fall", "autumn", "jesieÅ„"],
            },
        ),
        "timeOfDay": extract_percentage_width_data(
            soup,
            "timeOfDay",
            {
                "day": ["day", "dzieÅ„"],
                "night": ["night", "noc", "evening", "wieczÃ³r"],
            },
        ),
        "sillage": extract_voting_data(
            soup,
            "sillage",
            {
                "intimate": ["intimate"],
                "moderate": ["moderate"],
                "strong": ["strong"],
                "enormous": ["enormous"],
            },
        ),
    }


async def scrape_perfume_data(url: str, max_retries: int = 3, vpn_manager: Optional[VPNManager] = None) -> Dict[str, Any]:
    """GÅ‚Ã³wna funkcja scrapujÄ…ca dane o perfumach.
    
    Args:
        url: URL strony do scrapowania
        max_retries: Maksymalna liczba prÃ³b przy bÅ‚Ä™dach 429
        vpn_manager: Opcjonalny menedÅ¼er VPN
    """
    # Upewnij siÄ™, Å¼e VPN jest poÅ‚Ä…czony
    if vpn_manager:
        if not vpn_manager.is_connected():
            print("ğŸ”Œ ÅÄ…czenie z VPN przed scrapowaniem...")
            if not await vpn_manager.connect():
                print("âš ï¸  Nie udaÅ‚o siÄ™ poÅ‚Ä…czyÄ‡ z VPN, kontynuowanie bez VPN...", file=sys.stderr)
    
    for attempt in range(max_retries):
        try:
            # Generuj nowe losowe nagÅ‚Ã³wki dla kaÅ¼dej prÃ³by
            headers = get_random_headers()
            
            # Dodaj losowe opÃ³Åºnienie przed Å¼Ä…daniem (1-5 sekund)
            if attempt > 0:
                delay = random.uniform(2.0, 5.0)
                print(f"â³ Oczekiwanie {delay:.1f}s przed ponownÄ… prÃ³bÄ…...")
                await asyncio.sleep(delay)
            
            # UtwÃ³rz nowy crawler dla kaÅ¼dej prÃ³by (czyÅ›ci sesjÄ™ i cookies)
            async with AsyncWebCrawler(
                headless=True,
                verbose=False,
                # WyÅ‚Ä…cz cache i cookies aby uniknÄ…Ä‡ Å›ledzenia
                cache_enabled=False,
            ) as crawler:
                # UÅ¼yj networkidle z dÅ‚uÅ¼szym timeoutem i wiÄ™kszym opÃ³Åºnieniem
                # aby zapewniÄ‡ peÅ‚ne zaÅ‚adowanie JavaScript
                result = await crawler.arun(
                    url=url,
                    headers=headers,
                    wait_for="networkidle",  # Czekaj na zakoÅ„czenie Å‚adowania sieci
                    delay_before_return_html=0.0,  # Brak opÃ³Åºnienia - maksymalna prÄ™dkoÅ›Ä‡
                )
                
                # SprawdÅº czy otrzymaliÅ›my bÅ‚Ä…d 429
                if result.status_code == 429:
                    print(f"âš ï¸  Otrzymano bÅ‚Ä…d 429 (Too Many Requests). PrÃ³ba ponowna...", file=sys.stderr)
                    
                    # W przypadku bÅ‚Ä™du 429, zmieÅ„ konfiguracjÄ™ VPN i poczekaj dÅ‚uÅ¼ej
                    if vpn_manager:
                        print("ğŸ”„ Zmienianie konfiguracji VPN...", file=sys.stderr)
                        await vpn_manager.reconnect_with_new_config()
                        # DÅ‚uÅ¼sze oczekiwanie po zmianie VPN (5-10 sekund)
                        wait_time = random.uniform(5.0, 10.0)
                        print(f"â³ Oczekiwanie {wait_time:.1f}s po zmianie VPN...")
                        await asyncio.sleep(wait_time)
                    
                    continue  # SprÃ³buj ponownie
                
                if not result.success:
                    # SprawdÅº czy bÅ‚Ä…d zawiera informacjÄ™ o 429
                    if "429" in str(result.error_message) or "too many" in str(result.error_message).lower():
                        print(f"âš ï¸  Wykryto bÅ‚Ä…d 429. PrÃ³ba ponowna...", file=sys.stderr)
                        
                        # W przypadku bÅ‚Ä™du 429, zmieÅ„ konfiguracjÄ™ VPN i poczekaj dÅ‚uÅ¼ej
                        if vpn_manager:
                            print("ğŸ”„ Zmienianie konfiguracji VPN...", file=sys.stderr)
                            await vpn_manager.reconnect_with_new_config()
                            # DÅ‚uÅ¼sze oczekiwanie po zmianie VPN (5-10 sekund)
                            wait_time = random.uniform(5.0, 10.0)
                            print(f"â³ Oczekiwanie {wait_time:.1f}s po zmianie VPN...")
                            await asyncio.sleep(wait_time)
                        
                        continue
                    raise Exception(f"Nie udaÅ‚o siÄ™ pobraÄ‡ strony: {result.error_message}")
                
                # SprawdÅº czy strona zwrÃ³ciÅ‚a bÅ‚Ä…d 404 lub podobny
                html = result.html
                if html and is_404_error_page(html, getattr(result, 'status_code', None)):
                    # W przypadku bÅ‚Ä™du 404, zmieÅ„ VPN i sprÃ³buj ponownie
                    if vpn_manager:
                        print("ğŸ”„ Strona zwrÃ³ciÅ‚a bÅ‚Ä…d (404 lub podobny), zmienianie konfiguracji VPN...", file=sys.stderr)
                        await vpn_manager.reconnect_with_new_config()
                        # KrÃ³tsze oczekiwanie dla 404 (2-4 sekundy)
                        wait_time = random.uniform(2.0, 4.0)
                        await asyncio.sleep(wait_time)
                        continue
                    else:
                        raise Exception("Strona zwrÃ³ciÅ‚a bÅ‚Ä…d (404 lub podobny)")
                
                # JeÅ›li dotarliÅ›my tutaj, request byÅ‚ udany
                break
                
        except Exception as e:
            # JeÅ›li to ostatnia prÃ³ba, rzuÄ‡ wyjÄ…tek
            if attempt == max_retries - 1:
                raise
            
            # SprawdÅº czy bÅ‚Ä…d zawiera informacjÄ™ o 429 lub problemach z sieciÄ…
            error_str = str(e).lower()
            if "429" in error_str or "too many" in error_str or "rate limit" in error_str:
                print(f"âš ï¸  Wykryto bÅ‚Ä…d rate limiting. PrÃ³ba ponowna...", file=sys.stderr)
                
                # W przypadku bÅ‚Ä™du rate limiting, zmieÅ„ konfiguracjÄ™ VPN i poczekaj dÅ‚uÅ¼ej
                if vpn_manager:
                    print("ğŸ”„ Zmienianie konfiguracji VPN...", file=sys.stderr)
                    await vpn_manager.reconnect_with_new_config()
                    # DÅ‚uÅ¼sze oczekiwanie po zmianie VPN (5-10 sekund)
                    wait_time = random.uniform(5.0, 10.0)
                    print(f"â³ Oczekiwanie {wait_time:.1f}s po zmianie VPN...")
                    await asyncio.sleep(wait_time)
                
                continue
            elif "network" in error_str or "connection" in error_str or "timeout" in error_str:
                # W przypadku problemÃ³w z sieciÄ…, sprÃ³buj zmieniÄ‡ VPN
                if vpn_manager:
                    print("ğŸ”„ Problem z sieciÄ…, zmienianie konfiguracji VPN...", file=sys.stderr)
                    await vpn_manager.reconnect_with_new_config()
                    wait_time = random.uniform(3.0, 6.0)
                    await asyncio.sleep(wait_time)
                    continue
                else:
                    raise
            elif "404" in error_str or "not found" in error_str or "strona zwrÃ³ciÅ‚a bÅ‚Ä…d" in error_str:
                # W przypadku bÅ‚Ä™du 404, zmieÅ„ VPN i sprÃ³buj ponownie
                if vpn_manager:
                    print("ğŸ”„ Strona zwrÃ³ciÅ‚a bÅ‚Ä…d (404 lub podobny), zmienianie konfiguracji VPN...", file=sys.stderr)
                    await vpn_manager.reconnect_with_new_config()
                    wait_time = random.uniform(2.0, 4.0)
                    await asyncio.sleep(wait_time)
                    continue
                else:
                    raise
            else:
                # JeÅ›li to inny bÅ‚Ä…d, rzuÄ‡ wyjÄ…tek od razu
                raise
    
    # JeÅ›li dotarliÅ›my tutaj, oznacza to Å¼e request byÅ‚ udany
    # (gdyby wszystkie prÃ³by siÄ™ nie powiodÅ‚y, wyjÄ…tek zostaÅ‚by rzucony wczeÅ›niej)
    html = result.html
    soup = BeautifulSoup(html, "html.parser")
    
    # ZnajdÅº element #main-content - sprÃ³buj kilka razy z opÃ³Åºnieniem jeÅ›li nie znaleziono
    main_content = soup.find(id="main-content")
    if not main_content:
        # JeÅ›li nie znaleziono, sprÃ³buj uÅ¼yÄ‡ caÅ‚ego body jako fallback
        # lub sprawdÅº czy HTML w ogÃ³le zostaÅ‚ pobrany
        if not html or len(html) < 100:
            raise Exception(f"Nie udaÅ‚o siÄ™ pobraÄ‡ zawartoÅ›ci strony. HTML ma tylko {len(html) if html else 0} znakÃ³w.")
        
        # SprawdÅº czy strona zostaÅ‚a przekierowana lub czy jest bÅ‚Ä…d
        # (to sprawdzenie jest juÅ¼ wykonane w pÄ™tli retry, ale zostawiamy jako dodatkowe zabezpieczenie)
        if is_404_error_page(html):
            raise Exception("Strona zwrÃ³ciÅ‚a bÅ‚Ä…d (404 lub podobny)")
        
        # SprÃ³buj uÅ¼yÄ‡ body jako fallback
        body = soup.find("body")
        if body:
            print("âš ï¸  OstrzeÅ¼enie: Nie znaleziono #main-content, uÅ¼ywam body jako fallback", file=sys.stderr)
            main_content = body
        else:
            raise Exception("Nie znaleziono elementu #main-content ani body. Strona moÅ¼e wymagaÄ‡ JavaScript lub byÄ‡ zablokowana.")
    
    # UsuÅ„ niechciane elementy
    remove_unwanted_elements(main_content)
    
    # WyciÄ…gnij wszystkie dane (BEZ userReviews - bÄ™dÄ… w osobnym pliku)
    perfume_data = {
        "perfumeName": extract_perfume_name(main_content),
        "brand": extract_brand(main_content),
        "description": extract_description(main_content),
        "mainImageUrl": extract_main_image_url(main_content, url),
        "rating": extract_rating(main_content),
        "ratingCount": extract_rating_count(main_content),
        "notes": extract_notes(main_content),
        "similarPerfumes": extract_similar_perfumes(main_content),
        "recommendedPerfumes": extract_recommended_perfumes(main_content),
        "remindsMePerfumes": extract_reminds_me_perfumes(main_content),
        "pros": extract_pros(main_content),
        "cons": extract_cons(main_content),
    }
    
    # Dodaj dane gÅ‚osowania
    voting_data = extract_all_voting_data(main_content)
    perfume_data.update(voting_data)
    
    return perfume_data


async def main():
    """GÅ‚Ã³wna funkcja programu."""
    if len(sys.argv) > 1:
        url = sys.argv[1]
    else:
        url = input("Podaj URL strony do scrapowania: ").strip()
    
    if not url:
        print("BÅ‚Ä…d: URL nie moÅ¼e byÄ‡ pusty", file=sys.stderr)
        sys.exit(1)
    
    if not url.startswith(("http://", "https://")):
        url = "https://" + url
    
    print(f"Scrapowanie strony: {url}")
    
    try:
        data = await scrape_perfume_data(url)
        
        # Zapisz do output.js
        output_file = "output.js"
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"Dane zapisane do {output_file}")
        
        # Zapisz rÃ³wnieÅ¼ do output.json
        output_json_file = "output.json"
        with open(output_json_file, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"Dane zapisane do {output_json_file}")
        
        # Uruchom testy wartoÅ›ci LONGEVITY
        print("\n" + "=" * 50)
        print("Uruchamianie testÃ³w wartoÅ›ci LONGEVITY...")
        print("=" * 50)
        
        try:
            from test_output_longevity import test_longevity_values
            test_passed = test_longevity_values(output_file)
            if not test_passed:
                print("\nâš ï¸  OstrzeÅ¼enie: Testy wartoÅ›ci LONGEVITY nie przeszÅ‚y pomyÅ›lnie")
                print("   SprawdÅº dane w pliku output.js\n")
        except ImportError:
            print("âš ï¸  Nie moÅ¼na zaimportowaÄ‡ moduÅ‚u testowego")
        except Exception as e:
            print(f"âš ï¸  BÅ‚Ä…d podczas uruchamiania testÃ³w: {e}")
        
        # Uruchom testy wartoÅ›ci SILLAGE
        print("\n" + "=" * 50)
        print("Uruchamianie testÃ³w wartoÅ›ci SILLAGE...")
        print("=" * 50)
        
        try:
            from test_output_sillage import test_sillage_values
            test_passed = test_sillage_values(output_file)
            if not test_passed:
                print("\nâš ï¸  OstrzeÅ¼enie: Testy wartoÅ›ci SILLAGE nie przeszÅ‚y pomyÅ›lnie")
                print("   SprawdÅº dane w pliku output.js\n")
        except ImportError:
            print("âš ï¸  Nie moÅ¼na zaimportowaÄ‡ moduÅ‚u testowego SILLAGE")
        except Exception as e:
            print(f"âš ï¸  BÅ‚Ä…d podczas uruchamiania testÃ³w SILLAGE: {e}")
        
        # Uruchom testy wartoÅ›ci GENDER
        print("\n" + "=" * 50)
        print("Uruchamianie testÃ³w wartoÅ›ci GENDER...")
        print("=" * 50)
        
        try:
            from test_output_gender import test_gender_values
            test_passed = test_gender_values(output_file)
            if not test_passed:
                print("\nâš ï¸  OstrzeÅ¼enie: Testy wartoÅ›ci GENDER nie przeszÅ‚y pomyÅ›lnie")
                print("   SprawdÅº dane w pliku output.js\n")
        except ImportError:
            print("âš ï¸  Nie moÅ¼na zaimportowaÄ‡ moduÅ‚u testowego GENDER")
        except Exception as e:
            print(f"âš ï¸  BÅ‚Ä…d podczas uruchamiania testÃ³w GENDER: {e}")
        
        # Uruchom testy wartoÅ›ci PRICE VALUE
        print("\n" + "=" * 50)
        print("Uruchamianie testÃ³w wartoÅ›ci PRICE VALUE...")
        print("=" * 50)
        
        try:
            from test_output_price_value import test_price_value_values
            test_passed = test_price_value_values(output_file)
            if not test_passed:
                print("\nâš ï¸  OstrzeÅ¼enie: Testy wartoÅ›ci PRICE VALUE nie przeszÅ‚y pomyÅ›lnie")
                print("   SprawdÅº dane w pliku output.js\n")
        except ImportError:
            print("âš ï¸  Nie moÅ¼na zaimportowaÄ‡ moduÅ‚u testowego PRICE VALUE")
        except Exception as e:
            print(f"âš ï¸  BÅ‚Ä…d podczas uruchamiania testÃ³w PRICE VALUE: {e}")
        
        # Uruchom testy sekcji "People who like this also like"
        print("\n" + "=" * 50)
        print("Uruchamianie testÃ³w sekcji 'People who like this also like'...")
        print("=" * 50)
        
        try:
            from test_output_also_like import test_also_like_perfumes
            test_passed = test_also_like_perfumes(output_file)
            if not test_passed:
                print("\nâš ï¸  OstrzeÅ¼enie: Testy sekcji 'People who like this also like' nie przeszÅ‚y pomyÅ›lnie")
                print("   SprawdÅº dane w pliku output.js\n")
        except ImportError:
            print("âš ï¸  Nie moÅ¼na zaimportowaÄ‡ moduÅ‚u testowego 'People who like this also like'")
        except Exception as e:
            print(f"âš ï¸  BÅ‚Ä…d podczas uruchamiania testÃ³w 'People who like this also like': {e}")
        
        # Po zakoÅ„czeniu wszystkich testÃ³w, uruchom scrape_reviews.py
        print("\n" + "=" * 50)
        print("Uruchamianie scrape_reviews.py...")
        print("=" * 50)
        
        try:
            import subprocess
            result = subprocess.run(
                [sys.executable, "scrape_reviews.py", url],
                capture_output=True,
                text=True,
                encoding="utf-8"
            )
            if result.stdout:
                print(result.stdout)
            if result.stderr:
                print(result.stderr, file=sys.stderr)
            if result.returncode != 0:
                print(f"âš ï¸  BÅ‚Ä…d: scrape_reviews.py zakoÅ„czyÅ‚ siÄ™ z kodem {result.returncode}")
        except Exception as e:
            print(f"âš ï¸  BÅ‚Ä…d podczas uruchamiania scrape_reviews.py: {e}")
            import traceback
            traceback.print_exc()
        
    except Exception as e:
        print(f"BÅ‚Ä…d: {e}", file=sys.stderr)
        sys.exit(1)


if __name__ == "__main__":
    import asyncio
    
    asyncio.run(main())

