#!/usr/bin/env python3
"""
Program do scrapowania komentarzy/recenzji ze strony z sekcji #all-reviews.
Zapisuje komentarze do tablicy w pliku review.json.
"""

import json
import sys
import random
import asyncio
from typing import List, Optional, Dict

from bs4 import BeautifulSoup
from crawl4ai import AsyncWebCrawler
from vpn_manager import VPNManager


# Lista User-Agent do rotacji (r√≥≈ºne przeglƒÖdarki i systemy)
USER_AGENTS = [
    # Chrome na Windows
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
    # Chrome na macOS
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
    # Firefox na Windows
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:120.0) Gecko/20100101 Firefox/120.0",
    # Firefox na macOS
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:121.0) Gecko/20100101 Firefox/121.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:120.0) Gecko/20100101 Firefox/120.0",
    # Safari na macOS
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15",
    # Edge na Windows
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36 Edg/119.0.0.0",
]


def get_random_headers() -> Dict[str, str]:
    """Generuje losowe nag≈Ç√≥wki HTTP z rotacjƒÖ User-Agent."""
    user_agent = random.choice(USER_AGENTS)
    
    # R√≥≈ºne Accept-Language w zale≈ºno≈õci od User-Agent
    if "Firefox" in user_agent:
        accept_language = random.choice([
            "pl-PL,pl;q=0.9,en-US;q=0.8,en;q=0.7",
            "en-US,en;q=0.9",
            "pl-PL,pl;q=0.9",
        ])
    else:
        accept_language = random.choice([
            "pl-PL,pl;q=0.9,en-US;q=0.8,en;q=0.7",
            "en-US,en;q=0.9,pl;q=0.8",
            "pl-PL,pl;q=0.9",
        ])
    
    headers = {
        "User-Agent": user_agent,
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
        "Accept-Language": accept_language,
        "Accept-Encoding": "gzip, deflate, br",
        "DNT": "1",
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1",
        "Sec-Fetch-Dest": "document",
        "Sec-Fetch-Mode": "navigate",
        "Sec-Fetch-Site": random.choice(["none", "same-origin"]),
        "Sec-Fetch-User": "?1",
        "Cache-Control": random.choice(["max-age=0", "no-cache", "no-store"]),
        "Referer": random.choice([
            "https://www.google.com/",
            "https://www.google.pl/",
            "https://www.fragrantica.com/",
            "",
        ]),
    }
    
    return headers


def clean_text(text: str) -> str:
    """Usuwa bia≈Çe znaki i normalizuje tekst."""
    if not text:
        return ""
    return " ".join(text.split())


def extract_reviews(soup: BeautifulSoup) -> List[str]:
    """WyciƒÖga wszystkie recenzje z sekcji all-reviews jako tablicƒô string√≥w."""
    reviews = []
    
    # Szukaj element√≥w z itemprop="review" (recenzje sƒÖ w elementach fragrance-review-box)
    review_elems = soup.find_all(itemprop="review")
    
    for review_elem in review_elems:
        # WyciƒÖgnij tylko tekst recenzji
        review_body = review_elem.find(itemprop="reviewBody")
        if review_body:
            text = clean_text(review_body.get_text())
            if text and len(text) > 0:
                reviews.append(text)
    
    return reviews


async def scrape_reviews(url: str, vpn_manager: Optional[VPNManager] = None) -> List[str]:
    """G≈Ç√≥wna funkcja scrapujƒÖca recenzje.
    
    Args:
        url: URL strony do scrapowania
        vpn_manager: Opcjonalny mened≈ºer VPN
    """
    # Dodaj #all-reviews do URL je≈õli nie ma
    if "#all-reviews" not in url:
        url = url + "#all-reviews"
    
    # Upewnij siƒô, ≈ºe VPN jest po≈ÇƒÖczony
    if vpn_manager:
        if not vpn_manager.is_connected():
            print("üîå ≈ÅƒÖczenie z VPN przed scrapowaniem recenzji...")
            if not await vpn_manager.connect():
                print("‚ö†Ô∏è  Nie uda≈Ço siƒô po≈ÇƒÖczyƒá z VPN, kontynuowanie bez VPN...", file=sys.stderr)
    
    # Generuj losowe nag≈Ç√≥wki
    headers = get_random_headers()
    
    # Dodaj losowe op√≥≈∫nienie przed ≈ºƒÖdaniem (1-3 sekundy)
    delay = random.uniform(1.0, 3.0)
    await asyncio.sleep(delay)
    
    # Utw√≥rz nowy crawler (czy≈õci sesjƒô i cookies)
    async with AsyncWebCrawler(
        headless=True,
        verbose=False,
        # Wy≈ÇƒÖcz cache i cookies aby uniknƒÖƒá ≈õledzenia
        cache_enabled=False,
    ) as crawler:
        # U≈ºyj networkidle z d≈Çu≈ºszym timeoutem i wiƒôkszym op√≥≈∫nieniem
        # aby zapewniƒá pe≈Çne za≈Çadowanie JavaScript
        result = await crawler.arun(
            url=url,
            headers=headers,
            wait_for="networkidle",
            delay_before_return_html=0.0,  # Brak op√≥≈∫nienia - maksymalna prƒôdko≈õƒá
        )
        
        # Sprawd≈∫ czy otrzymali≈õmy b≈ÇƒÖd 429
        if result.status_code == 429:
            print("‚ö†Ô∏è  Otrzymano b≈ÇƒÖd 429 (Too Many Requests).", file=sys.stderr)
            if vpn_manager:
                print("üîÑ Zmienianie konfiguracji VPN...", file=sys.stderr)
                await vpn_manager.reconnect_with_new_config()
                # D≈Çu≈ºsze oczekiwanie po zmianie VPN (5-10 sekund)
                wait_time = random.uniform(5.0, 10.0)
                print(f"‚è≥ Oczekiwanie {wait_time:.1f}s po zmianie VPN...")
                await asyncio.sleep(wait_time)
            raise Exception("B≈ÇƒÖd 429: Too Many Requests")
        
        if not result.success:
            # Sprawd≈∫ czy b≈ÇƒÖd zawiera informacjƒô o 429
            if "429" in str(result.error_message) or "too many" in str(result.error_message).lower():
                if vpn_manager:
                    print("üîÑ Zmienianie konfiguracji VPN...", file=sys.stderr)
                    await vpn_manager.reconnect_with_new_config()
                    # D≈Çu≈ºsze oczekiwanie po zmianie VPN (5-10 sekund)
                    wait_time = random.uniform(5.0, 10.0)
                    print(f"‚è≥ Oczekiwanie {wait_time:.1f}s po zmianie VPN...")
                    await asyncio.sleep(wait_time)
            raise Exception(f"Nie uda≈Ço siƒô pobraƒá strony: {result.error_message}")
        
        html = result.html
        soup = BeautifulSoup(html, "html.parser")
        
        # WyciƒÖgnij wszystkie recenzje
        reviews = extract_reviews(soup)
        
        return reviews


async def main():
    """G≈Ç√≥wna funkcja programu."""
    if len(sys.argv) > 1:
        url = sys.argv[1]
    else:
        url = input("Podaj URL strony do scrapowania: ").strip()
    
    if not url:
        print("B≈ÇƒÖd: URL nie mo≈ºe byƒá pusty", file=sys.stderr)
        sys.exit(1)
    
    if not url.startswith(("http://", "https://")):
        url = "https://" + url
    
    print(f"Scrapowanie recenzji ze strony: {url}")
    
    try:
        reviews = await scrape_reviews(url)
        
        # Zapisz do output.json (dodaj recenzje do istniejƒÖcych danych)
        output_file = "output.json"
        
        # Za≈Çaduj istniejƒÖce dane z output.json je≈õli istnieje
        existing_data = {}
        try:
            with open(output_file, "r", encoding="utf-8") as f:
                existing_data = json.load(f)
        except FileNotFoundError:
            print(f"‚ö†Ô∏è  Plik {output_file} nie istnieje, tworzenie nowego pliku")
        except json.JSONDecodeError:
            print(f"‚ö†Ô∏è  B≈ÇƒÖd parsowania {output_file}, nadpisanie pliku")
        
        # Dodaj recenzje do danych jako klucz "review"
        existing_data["review"] = reviews
        
        # Zapisz zaktualizowane dane do output.json
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(existing_data, f, ensure_ascii=False, indent=2)
        
        print(f"‚úì Znaleziono {len(reviews)} recenzji")
        print(f"‚úì Recenzje zapisane do {output_file}")
        
    except Exception as e:
        print(f"B≈ÇƒÖd: {e}", file=sys.stderr)
        sys.exit(1)


if __name__ == "__main__":
    import asyncio
    
    asyncio.run(main())

